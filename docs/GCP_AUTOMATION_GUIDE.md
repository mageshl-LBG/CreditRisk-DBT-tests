# GCP Automation & Reporting Guide

This guide explains how to operationalize the automated pipelines in Google Cloud Platform (GCP) using Cloud Composer (Airflow) while maintaining visibility into run reporting.

## 1. Automation Architecture

**Tool**: Cloud Composer (Managed Airflow)
**Goal**: Run DBT models automatically based on upstream data availability.

### The Generated DAG Structure
The `GCPDeploymentView` generates Python DAGs with two key components:

1.  **Sensors (`BigQueryTableExistenceSensor`)**:
    -   These "listen" for your Source Tables (ODP).
    -   The pipeline **only starts** when the source data arrives for the day.
    -   *Benefit*: No wasted compute running on empty data.

2.  **Transformation Task (`BigQueryInsertJobOperator`)**:
    -   Executes the SQL logic generated by the framework.
    -   Embeds the 12-point Quality Checks directly into the query logic.

## 2. Deploying the DAGs

1.  **Generate**: Open the App > **GCP Deployment** > Click **Generate DAG** for your FDP/CDP asset.
2.  **Copy**: Copy the Python code.
3.  **Deploy**: Upload the `.py` file to your Cloud Composer's DAG bucket (e.g., `gs://us-central1-composer-env-.../dags/`).
4.  **Verify**: Open the Airflow UI. The DAG will appear (e.g., `datatrust_fdp_customer_360`).

## 3. Reporting & Monitoring

Since the logic is running in Airflow, you can leverage native reporting + BigQuery logging.

### Method A: Airflow UI Reporting
-   **Grid View**: Shows the daily status (Green=Success, Red=Fail) of every run.
-   **Gantt Chart**: Shows duration of each step.

### Method B: Automated Run Reporting (Recommended)

To report on "Number of times ran", query the Airflow metadata database or use BigQuery audit logs.

**SQL Query to Count Runs (Run in BigQuery):**

```sql
SELECT
    protopayload_auditlog.resourceName AS dag_id,
    COUNT(*) as run_count,
    MIN(timestamp) as first_run,
    MAX(timestamp) as last_run
FROM `region-us.logging.googleapis.com/projects/YOUR_PROJECT/locations/global/buckets/_Default/_views/_AllLogs`
WHERE
    timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
    AND resource.type = "composer_environment"
    AND textPayload LIKE "%TaskInstance: %"
GROUP BY 1
ORDER BY 2 DESC
```

### Method C: Custom Email Alerts
The generated DAG includes default email alerts:
```python
default_args = {
    'email_on_failure': True,
    'email': ['your-team@company.com'], # Update this in TemplateService
}
```
This ensures you are notified immediately of failures without manual checking.
