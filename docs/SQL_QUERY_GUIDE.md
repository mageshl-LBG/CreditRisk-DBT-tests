# SQL Query Documentation Guide

## Purpose
This document explains every SQL query generated by the DataTrust platform, helping you understand what each test does and how to read the SQL code.

---

## Table of Contents
1. [Understanding SQL Basics](#understanding-sql-basics)
2. [Test Type Queries](#test-type-queries)
3. [Query Components Explained](#query-components-explained)

---

## Understanding SQL Basics

### What is SQL?
SQL (Structured Query Language) is used to interact with databases. Think of it as asking questions to your data.

### Basic SQL Structure:
```sql
SELECT column_name    -- What data do you want?
FROM table_name       -- Where is the data?
WHERE condition       -- What filter should apply?
```

### Common SQL Keywords:
- **SELECT**: Choose which columns to retrieve
- **FROM**: Specify which table to query
- **WHERE**: Filter rows based on conditions
- **COUNT(*)**: Count the number of rows
- **SUM()**: Add up values in a column
- **GROUP BY**: Group rows that have the same values
- **HAVING**: Filter groups (like WHERE but for groups)

---

## Test Type Queries

### 1. UNIQUE PRIMARY KEY TEST

**Purpose**: Ensures no duplicate IDs exist in your table

**SQL Query**:
```sql
-- Test: Unique Primary Key
SELECT 
  'customer_table' as table_name,           -- Name of table being tested
  'Unique Primary Key' as test_type,        -- Type of test
  COUNT(*) as duplicate_count,              -- How many duplicates found
  ARRAY_AGG(STRUCT(id) LIMIT 5) as sample_duplicates  -- Show first 5 duplicates
FROM (
  SELECT id, COUNT(*) as cnt                -- Count occurrences of each ID
  FROM `project.dataset.customer_table`     -- Your table
  GROUP BY id                               -- Group by ID to find duplicates
  HAVING cnt > 1                            -- Only show IDs that appear more than once
)
```

**What it checks**:
- ✅ **PASS**: duplicate_count = 0 (no duplicates)
- ❌ **FAIL**: duplicate_count > 0 (duplicates exist)

**How to read results**:
- `duplicate_count`: Number of duplicate IDs found
- `sample_duplicates`: Examples of duplicate IDs

---

### 2. NOT NULL CHECK

**Purpose**: Ensures required fields are never empty

**SQL Query**:
```sql
-- Test: Not Null Check
SELECT 
  'customer_table' as table_name,           -- Table being tested
  'Not Null' as test_type,                  -- Test type
  COUNT(*) as null_count,                   -- Total rows with nulls
  COUNTIF(id IS NULL) as id_nulls,          -- Nulls in id column
  COUNTIF(created_at IS NULL) as created_at_nulls  -- Nulls in created_at
FROM `project.dataset.customer_table`       -- Your table
WHERE id IS NULL OR created_at IS NULL      -- Find rows where either field is null
```

**What it checks**:
- ✅ **PASS**: null_count = 0 (no nulls in required fields)
- ❌ **FAIL**: null_count > 0 (some required fields are null)

**How to read results**:
- `null_count`: Total rows with at least one null
- `id_nulls`: How many rows have null ID
- `created_at_nulls`: How many rows have null timestamp

---

### 3. ROW COUNT RECONCILIATION

**Purpose**: Ensures source and target tables have the same number of rows

**SQL Query**:
```sql
-- Test: Row Count Reconciliation
WITH source_count AS (
  SELECT COUNT(*) as cnt                    -- Count rows in source
  FROM `project.dataset.source_table`
),
target_count AS (
  SELECT COUNT(*) as cnt                    -- Count rows in target
  FROM `project.dataset.target_table`
)
SELECT 
  'target_table' as table_name,             -- Table being tested
  'Row Count Reconciliation' as test_type,  -- Test type
  s.cnt as source_count,                    -- Rows in source
  t.cnt as target_count,                    -- Rows in target
  t.cnt - s.cnt as difference,              -- Difference between them
  ROUND(ABS(t.cnt - s.cnt) * 100.0 / NULLIF(s.cnt, 0), 2) as variance_percent
FROM source_count s, target_count t
```

**What it checks**:
- ✅ **PASS**: difference = 0 (same row count)
- ⚠️ **WARNING**: difference < 5% (small variance)
- ❌ **FAIL**: difference > 5% (significant variance)

**How to read results**:
- `source_count`: Number of rows in source table
- `target_count`: Number of rows in target table
- `difference`: How many rows are missing/extra
- `variance_percent`: Percentage difference

**Example**:
- Source: 1000 rows
- Target: 950 rows
- Difference: -50 (50 rows missing)
- Variance: 5% (acceptable warning threshold)

---

### 4. SCHEMA VALIDATION

**Purpose**: Verifies that expected columns exist with correct data types

**SQL Query**:
```sql
-- Test: Schema Validation
SELECT 
  'customer_table' as table_name,           -- Table being validated
  'Schema Valid' as test_type,              -- Test type
  column_name,                              -- Name of each column
  data_type,                                -- Data type (STRING, INT64, etc.)
  is_nullable                               -- Can it contain nulls? (YES/NO)
FROM `project.dataset`.INFORMATION_SCHEMA.COLUMNS  -- System table with schema info
WHERE table_name = 'customer_table'         -- Filter to our table
  AND column_name IN ('id', 'name', 'created_at')  -- Expected columns
ORDER BY ordinal_position                   -- Order by column position
```

**What it checks**:
- ✅ **PASS**: All expected columns exist with correct types
- ❌ **FAIL**: Missing columns or wrong data types

**How to read results**:
Each row shows one column with:
- `column_name`: Column name
- `data_type`: Expected type (STRING, INT64, TIMESTAMP, etc.)
- `is_nullable`: YES if nulls allowed, NO if required

---

### 5. VOLUME THRESHOLD

**Purpose**: Ensures table has minimum expected number of rows

**SQL Query**:
```sql
-- Test: Volume Threshold
WITH current_count AS (
  SELECT COUNT(*) as row_count              -- Count current rows
  FROM `project.dataset.customer_table`
)
SELECT 
  'customer_table' as table_name,           -- Table being tested
  'Volume Threshold' as test_type,          -- Test type
  row_count,                                -- Current row count
  1000 as min_threshold,                    -- Minimum expected rows
  CASE 
    WHEN row_count < 1000 THEN 'FAIL'       -- Less than minimum = FAIL
    ELSE 'PASS'                             -- At or above minimum = PASS
  END as status
FROM current_count
```

**What it checks**:
- ✅ **PASS**: row_count >= min_threshold
- ❌ **FAIL**: row_count < min_threshold

**How to read results**:
- `row_count`: Actual number of rows
- `min_threshold`: Minimum acceptable rows
- `status`: PASS or FAIL

---

### 6. FRESHNESS SLA

**Purpose**: Ensures data is recent and not stale

**SQL Query**:
```sql
-- Test: Data Freshness
SELECT 
  'customer_table' as table_name,           -- Table being tested
  'Freshness SLA' as test_type,             -- Test type
  MAX(updated_at) as latest_update,         -- Most recent update timestamp
  TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), MAX(updated_at), HOUR) as age_hours,
  24 as sla_hours,                          -- Maximum acceptable age (24 hours)
  CASE 
    WHEN TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), MAX(updated_at), HOUR) > 24 
    THEN 'FAIL'                             -- Data older than 24 hours = FAIL
    ELSE 'PASS'                             -- Data within 24 hours = PASS
  END as status
FROM `project.dataset.customer_table`
```

**What it checks**:
- ✅ **PASS**: age_hours <= sla_hours (data is fresh)
- ❌ **FAIL**: age_hours > sla_hours (data is stale)

**How to read results**:
- `latest_update`: When data was last updated
- `age_hours`: How many hours ago data was updated
- `sla_hours`: Maximum acceptable age
- `status`: PASS or FAIL

**Example**:
- Latest update: 2025-12-23 10:00:00
- Current time: 2025-12-24 10:00:00
- Age: 24 hours
- SLA: 24 hours
- Status: PASS (just within threshold)

---

### 7. DATA TYPE CONSISTENCY

**Purpose**: Ensures columns have consistent data types

**SQL Query**:
```sql
-- Test: Data Type Consistency
SELECT 
  'customer_table' as table_name,           -- Table being tested
  'Data Type Match' as test_type,           -- Test type
  column_name,                              -- Column name
  data_type,                                -- Current data type
  COUNT(*) as type_violations               -- Number of inconsistencies
FROM `project.dataset`.INFORMATION_SCHEMA.COLUMNS
WHERE table_name = 'customer_table'
GROUP BY column_name, data_type
```

**What it checks**:
- ✅ **PASS**: Each column has exactly one data type
- ❌ **FAIL**: Columns have mixed or unexpected types

---

### 8. STATISTICAL RANGE ANALYSIS

**Purpose**: Detects outliers and unusual values in numeric columns

**SQL Query**:
```sql
-- Test: Statistical Range Analysis
SELECT 
  'order_table' as table_name,              -- Table being tested
  'Stat Range' as test_type,                -- Test type
  MIN(amount) as amount_min,                -- Smallest value
  MAX(amount) as amount_max,                -- Largest value
  AVG(amount) as amount_avg,                -- Average value
  STDDEV(amount) as amount_stddev           -- Standard deviation
FROM `project.dataset.order_table`
```

**What it checks**:
- Identifies minimum and maximum values
- Calculates average to find typical values
- Uses standard deviation to detect outliers

**How to read results**:
- `amount_min`: Lowest amount (check for negatives or zeros)
- `amount_max`: Highest amount (check for unrealistic values)
- `amount_avg`: Average amount (typical transaction)
- `amount_stddev`: Spread of values (high = lots of variation)

**Example**:
- Min: $0.01
- Max: $999,999.00 (suspicious - possible outlier)
- Avg: $45.50
- StdDev: $125.00 (high variation)

---

### 9. CROSS-LAYER METRIC RECONCILIATION

**Purpose**: Ensures aggregated metrics match between layers

**SQL Query**:
```sql
-- Test: Cross-Layer Metric Reconciliation
WITH source_metrics AS (
  SELECT SUM(amount) as amount_sum          -- Sum in source layer
  FROM `project.dataset.fdp_orders`
),
target_metrics AS (
  SELECT SUM(amount) as amount_sum          -- Sum in target layer
  FROM `project.dataset.cdp_order_summary`
)
SELECT 
  'cdp_order_summary' as table_name,        -- Table being tested
  'Cross Layer Metric' as test_type,        -- Test type
  s.amount_sum as source_amount,            -- Source total
  t.amount_sum as target_amount,            -- Target total
  t.amount_sum - s.amount_sum as amount_diff  -- Difference
FROM source_metrics s, target_metrics t
```

**What it checks**:
- ✅ **PASS**: amount_diff = 0 (totals match exactly)
- ⚠️ **WARNING**: amount_diff < 1% (small rounding difference)
- ❌ **FAIL**: amount_diff > 1% (significant discrepancy)

**How to read results**:
- `source_amount`: Total in source layer
- `target_amount`: Total in target layer
- `amount_diff`: Difference (should be 0 or very small)

---

## Query Components Explained

### WITH Clause (Common Table Expression)
```sql
WITH temp_table AS (
  SELECT column FROM table
)
```
**Purpose**: Creates a temporary result set you can reference later
**Think of it as**: Creating a temporary mini-table to use in your main query

### CASE Statement
```sql
CASE 
  WHEN condition THEN 'result1'
  ELSE 'result2'
END
```
**Purpose**: If-then-else logic in SQL
**Example**: 
```sql
CASE 
  WHEN age < 18 THEN 'Minor'
  WHEN age >= 18 THEN 'Adult'
END
```

### COUNTIF Function
```sql
COUNTIF(column IS NULL)
```
**Purpose**: Count rows where condition is true
**Example**: `COUNTIF(age > 18)` counts how many people are adults

### TIMESTAMP_DIFF
```sql
TIMESTAMP_DIFF(timestamp1, timestamp2, HOUR)
```
**Purpose**: Calculate difference between two timestamps
**Units**: HOUR, DAY, MINUTE, SECOND
**Example**: `TIMESTAMP_DIFF('2025-12-24', '2025-12-23', DAY)` = 1 day

### NULLIF
```sql
NULLIF(value, 0)
```
**Purpose**: Prevents division by zero
**How it works**: Returns NULL if value equals 0, otherwise returns value
**Example**: `10 / NULLIF(count, 0)` won't crash if count is 0

### ARRAY_AGG
```sql
ARRAY_AGG(column LIMIT 5)
```
**Purpose**: Collect multiple values into an array
**Example**: `ARRAY_AGG(name LIMIT 5)` creates array of first 5 names

---

## Reading Test Results

### Status Indicators
- **PASS** ✅: Test succeeded, no issues found
- **WARNING** ⚠️: Minor issues, review recommended
- **FAIL** ❌: Critical issues, must fix

### Common Metrics
- **Count**: Number of rows/items
- **Percentage**: Ratio expressed as 0-100
- **Difference**: Subtraction result (can be negative)
- **Variance**: How much values differ from expected

---

## Tips for Beginners

1. **Read queries from inside out**: Start with innermost SELECT, work outward
2. **Look for WITH clauses first**: These create temporary tables used later
3. **Check WHERE conditions**: These filter which rows are included
4. **Understand aggregations**: COUNT, SUM, AVG combine multiple rows
5. **Watch for CASE statements**: These add conditional logic

---

## Example: Reading a Complete Query

```sql
-- Step 1: Create temporary table with source count
WITH source_count AS (
  SELECT COUNT(*) as cnt
  FROM source_table
),
-- Step 2: Create temporary table with target count
target_count AS (
  SELECT COUNT(*) as cnt
  FROM target_table
)
-- Step 3: Compare the two counts
SELECT 
  s.cnt as source_rows,      -- Rows from source
  t.cnt as target_rows,      -- Rows from target
  t.cnt - s.cnt as diff      -- Calculate difference
FROM source_count s, target_count t
```

**Reading order**:
1. First WITH creates `source_count` with row count from source
2. Second WITH creates `target_count` with row count from target
3. Final SELECT compares the two counts
4. Result shows if row counts match

---

*This guide covers all SQL queries generated by DataTrust. For more help, refer to the Implementation Guide.*
